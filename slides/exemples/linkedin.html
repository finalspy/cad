<!-- Slides pour LinkedIn -->
<section data-background-image="/public/backgrounds/linkedin.webp" data-background-size="cover">
    <aside class="notes">
        <ul>
            <li>
                LinkedIn a migr√© vers une architecture microservices pour am√©liorer la scalabilit√© et la performance.
            </li>
            <li>
                La migration a permis de r√©duire la latence et d'optimiser les connexions r√©seau.
            </li>
            <li>
                L'utilisation de HTTP/2 a permis de r√©duire le nombre de connexions simultan√©es.
            </li>
            <li>
                La gestion des flux a √©t√© am√©lior√©e gr√¢ce √† une meilleure int√©gration avec Netty.
            </li>
            <li>
                R√©sultat : une infrastructure plus performante et fluide.
            </li>
            <li>
                La migration a √©t√© progressive, permettant de tester et d'it√©rer sur les nouvelles fonctionnalit√©s.
            </li>
            <li>
                LinkedIn a √©galement mis en place des outils de monitoring pour suivre les performances et d√©tecter les probl√®mes.
            </li>
        </ul>
    </aside>
</section>

<section>
  <h2>LinkedIn ‚Äî Point de d√©part</h2>
  <p>Backend monolithique sur HTTP/1.1 avec limitations r√©seau.</p>
</section>
<section>
  <h2>‚ö°Ô∏è Pain points</h2>
  <ul>
    <li>Connexion excessive</li>
    <li>Temps de r√©ponse √©lev√©s</li>
    <li>Garbage collection fr√©quente</li>
  </ul>
</section>
<section>
  <h2>üåü Objectifs</h2>
  <ul>
    <li>Optimisation du transport r√©seau</li>
    <li>Am√©liorer la scalabilit√©</li>
  </ul>
</section>
<section>
  <h2>üõ†Ô∏è Solution</h2>
  <ul>
    <li>Migration vers HTTP/2</li>
    <li>Am√©lioration de Netty et gestion de flux</li>
  </ul>
</section>
<section>
  <h2>üìä R√©sultats chiffr√©s</h2>
  <ul>
    <li>Moins de connexions simultan√©es</li>
    <li>Latence r√©duite</li>
    <li>Moins de temps de GC</li>
  </ul>
</section>
<section>
  <h2>üåü R√©sultat</h2>
  <p>Infrastructure plus performante et fluide.</p>
</section>

<section>
  <h2>Incident / D√©clencheur : LinkedIn</h2>
  <p>Saturation du monolithe face √† la croissance explosive des donn√©es et des usages</p>
  <ul>
    <li>2009 : explosion des volumes de donn√©es (profil, feed, recherche)</li>
    <li>D√©pendance √† une base Oracle centralis√©e</li>
    <li>Difficult√©s √† tenir la charge et √† it√©rer rapidement</li>
  </ul>
  <aside class="notes">
    <ul>
      <li>LinkedIn d√©passait d√©j√† 50 millions de membres en 2009, et franchira les 100 millions en 2011</li>
      <li>Le backend monolithique Java reposait fortement sur une base Oracle pour tout : messages, relations, profils</li>
      <li>Les performances de recherche et de mise √† jour devenaient probl√©matiques</li>
    </ul>
  </aside>
</section>

<section>
  <h2>Nature du probl√®me</h2>
  <p>Scalabilit√© limit√©e et goulots d'√©tranglement dans les composants critiques</p>
  <ul>
    <li>Base Oracle satur√©e : requ√™tes longues et blocantes</li>
    <li>Monolithe trop rigide pour innover rapidement</li>
    <li>Pas de m√©canisme de propagation d'√©v√©nements entre modules</li>
  </ul>
  <aside class="notes">
    <ul>
      <li>Des √©quipes voulaient d√©ployer des nouveaut√©s (feed, notifications) mais les d√©pendances bloquaient tout</li>
      <li>Une modification dans le profil utilisateur pouvait casser la recherche ou les suggestions</li>
    </ul>
  </aside>
</section>

<section>
  <h2>Objectifs vis√©s</h2>
  <p>Repenser l'architecture pour la rendre scalable, modulaire et orient√©e √©v√©nement</p>
  <ul>
    <li>D√©coupler les composants via des syst√®mes de messaging</li>
    <li>Supporter des pics de charge (recherches, notifications)</li>
    <li>Permettre le scaling ind√©pendant des services</li>
  </ul>
  <aside class="notes">
    <ul>
      <li>LinkedIn vise une architecture o√π chaque module (feed, profil, messagerie) peut √©voluer seul</li>
      <li>L'objectif est aussi de permettre un traitement asynchrone massif</li>
    </ul>
  </aside>
</section>

<section>
  <h2>Solution mise en ≈ìuvre</h2>
  <p>Mise en place d'une architecture orient√©e √©v√©nements + microservices</p>
  <ul>
    <li>Cr√©ation de Kafka (2011) pour la diffusion d'√©v√©nements en temps r√©el</li>
    <li>D√©veloppement de Samza (2013) pour le traitement distribu√© de flux</li>
    <li>Transition vers des services sp√©cialis√©s : feed, profil, recherche, messagerie‚Ä¶</li>
  </ul>
  <aside class="notes">
    <ul>
      <li>Kafka devient la colonne vert√©brale de la communication inter-services</li>
      <li>Chaque √©v√©nement (nouvelle relation, update de profil) est publi√© et trait√© de mani√®re ind√©pendante</li>
      <li>Samza permet de faire du streaming analytics en ligne pour l'engagement, le ranking du feed‚Ä¶</li>
    </ul>
  </aside>
</section>

<section>
  <h2>R√©sultats observ√©s</h2>
  <p>Des performances, une modularit√© et une robustesse accrues</p>
  <ul>
    <li>Kafka traite aujourd'hui plus de <strong>7 000 milliards de messages par jour</strong></li>
    <li>Des centaines de microservices consomment les flux en parall√®le</li>
    <li>Feed personnalis√©, alertes, stats temps r√©el : tous pilot√©s par √©v√©nement</li>
  </ul>
  <aside class="notes">
    <ul>
      <li>Kafka est utilis√© pour la persistance, le log, le traitement de flux, la r√©plication de donn√©es</li>
      <li>Architecture plus r√©siliente : chaque service peut chuter ou red√©marrer sans bloquer l'√©cosyst√®me</li>
      <li>LinkedIn open-sourcera Kafka (2011), Samza (2013), Voldemort (cl√©-valeur), Brooklin (2017)...</li>
    </ul>
  </aside>
</section>

<section>
  <h2>En r√©sum√©</h2>
  <p>Un virage pionnier vers l'event-driven architecture dans un contexte Big Data</p>
  <ul>
    <li>Saturation du monolithe Oracle en 2009 ‚Üí refonte radicale</li>
    <li>Cr√©ation d'outils devenus des standards open source (Kafka, Samza‚Ä¶)</li>
    <li>Scalabilit√© et innovation accrues dans les services m√©tier</li>
  </ul>
  <aside class="notes">
    <ul>
      <li>LinkedIn est devenu un pionnier de l'event streaming √† l'√©chelle internet</li>
      <li>La logique asynchrone a permis de mieux absorber les pics de charge (notamment apr√®s l'acquisition par Microsoft en 2016)</li>
    </ul>
  </aside>
</section>

<section>
  <h2>LinkedIn - Pour aller plus loin</h2>
  <ul>
    <li><a href="https://engineering.linkedin.com/" target="_blank">LinkedIn Engineering - Blog officiel</a></li>
    <li><a href="https://engineering.linkedin.com/kafka/running-kafka-scale" target="_blank">Running Kafka at Scale - LinkedIn Engineering</a></li>
    <li><a href="https://engineering.linkedin.com/kafka/kafka-linkedin-current-and-future" target="_blank">Kafka at LinkedIn: Current and Future</a></li>
    <li><a href="https://www.slideshare.net/slideshow/apache-kafka-at-linkedin-how-linkedin-customizes-kafka-to-work-at-the-trillion-scale/229555971" target="_blank">Pr√©sentation - Apache Kafka at LinkedIn (SlideShare)</a></li>
    <li><a href="https://samza.apache.org/">Apache Samza (LinkedIn)</a></li>
  </ul>
  <aside class="notes">
    <ul>
      <li>Le blog LinkedIn Engineering propose des articles d√©taill√©s sur l'architecture, l'infrastructure et les innovations techniques de l'entreprise.</li>
      <li>Les articles sur Kafka d√©crivent comment LinkedIn utilise et adapte Apache Kafka pour g√©rer des volumes massifs de donn√©es en temps r√©el.</li>
      <li>La pr√©sentation sur SlideShare offre un aper√ßu de la personnalisation de Kafka par LinkedIn pour r√©pondre √† ses besoins sp√©cifiques.</li>
    </ul>
  </aside>
</section>

