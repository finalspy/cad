<!-- Slide d'introduction g√©n√©rale -->
<section data-background-image="/public/images/titre_concepts.jpg" 
          data-background-size="cover"
          data-background-opacity="0.5">
  <h2>Concepts cl√©s des applications distribu√©es</h2>
  <ul>
    <li>üî∫ Th√©or√®mes fondamentaux</li>
    <li>üß© Architecture et Design</li>
    <li><b>üõ°Ô∏è R√©silience et Scalabilit√©</b></li>
    <li>üß™ Tests et Observabilit√©</li>
    <li>üß† Gouvernance et √âquipes</li>
  </ul>
  <aside class="notes">
    Voici les cinq grands ensembles qui structurent ce module. Nous allons explorer chaque groupe de concepts √† travers des exemples concrets.
  </aside>
</section>

<!-- === SECTION 3 : R√©silience & Scalabilit√© === -->
<section>
  <h2>üõ°Ô∏è R√©silience et Scalabilit√©</h2>
  <aside class="notes">
    Objectif : garantir une continuit√© de service face aux pics de trafic et aux pannes partielles, en minimisant l‚Äôimpact sur l‚Äôutilisateur.  
    Principes cl√©s : ¬´ design for failure ¬ª, isolation des composants, √©lasticit√© et observabilit√©.
  </aside>
</section>
<section>
  <h2>Scalabilit√©</h2>
  <li><strong>Types de scalabilit√©</strong>
    <ul>
      <li><em>Verticale</em> : augmentation des ressources (CPU, RAM) d‚Äôun seul n≈ìud ‚Äî simple mais limit√©e par le mat√©riel.</li>
      <li><em>Horizontale</em> : ajout de nouveaux n≈ìuds identiques ‚Äî limite th√©orique forte, n√©cessite r√©partition de charge et coh√©rence.</li>
      <li><em>√âlastique</em> : ajustement automatique en fonction de la charge (mont√©e et descente) ‚Äî optimise co√ªt et performance.</li>
    </ul>
  </li>
  <aside class="notes">
    Historiquement, la scalabilit√© verticale √©tait privil√©gi√©e pour sa simplicit√©.
    Cependant, elle est limit√©e par la capacit√© physique des machines, et la fin de la loi de Moore.
    La scalabilit√© horizontale est souvent privil√©gi√©e dans les architectures cloud-native, car elle permet de g√©rer des charges de travail variables.  
    L'√©lasticit√© est essentielle pour optimiser les co√ªts dans le cloud, o√π vous ne payez que pour ce que vous utilisez.
  </aside>
</section>
<section>
  <h2>Scalabilit√©</h2>
  <li><strong>M√©canismes cloud-native</strong>
    <ul>
      <li><em>Auto-scaling</em> : r√®gles bas√©es sur m√©triques (CPU, latence, file d‚Äôattente).</li>
      <li><em>Load balancing</em> : DNS round-robin, appliances mat√©rielles, √©quilibreurs logiciels (NGINX, ELB).</li>
      <li><em>Sharding / Partitioning</em> : d√©coupage des donn√©es selon cl√©, plage ou hash pour r√©partir la charge I/O.</li>
      <li><em>Caching distribu√©</em> : CDN, Redis/Memcached en cluster pour soulager la base.</li>
    </ul>
  </li>
  <aside class="notes">
    L'auto-scaling est un m√©canisme cl√© pour g√©rer les pics de charge sans intervention humaine.  
    Le load balancing est essentiel pour r√©partir la charge entre plusieurs instances, garantissant ainsi une haute disponibilit√©.
    Le sharding et le partitioning sont cruciaux pour les bases de donn√©es, permettant de r√©partir la charge et d'am√©liorer la performance.  
    Le caching distribu√© r√©duit la latence et la charge sur les bases de donn√©es, en stockant les donn√©es fr√©quemment demand√©es.
  </aside>
</section>
<section>
  <h2>Scalabilit√©</h2>
  <li><strong>Strat√©gies avanc√©es</strong>
    <ul>
      <li><em>Pattern CQRS</em> : s√©paration des lectures et √©critures pour optimiser chaque flux.</li>
      <li><em>Event sourcing</em> : stockage des √©v√©nements pour reconstruire l‚Äô√©tat, faciliter la r√©plication.</li>
      <li><em>Serverless / FaaS</em> : ex√©cution √† la demande, facturation √† l‚Äôusage, mont√©e en charge transparente.</li>
    </ul>
  </li>
  <aside class="notes">
    CQRS et event sourcing sont souvent utilis√©s ensemble pour optimiser la performance et la tra√ßabilit√© des donn√©es.  
    Le serverless est id√©al pour des charges de travail impr√©visibles, mais n√©cessite une bonne gestion des d√©pendances et des latences.
    Il est important de bien comprendre les limites de chaque service serverless, notamment en termes de temps d'ex√©cution et de ressources.
  </aside>
</section>
<section>
  <h2>Scalabilit√©</h2>
  <li><strong>Aspects op√©rationnels</strong>
    <ul>
      <li>Co√ªt vs performance : choisir le bon ratio instances r√©serv√©es / √† la demande.</li>
      <li>Monitoring & alerting : CPU, latence, taux d‚Äôerreur, file d‚Äôattente.</li>
      <li>Tests de charge r√©guliers pour valider la dimensionnement.</li>
    </ul>
  </li>
  <aside class="notes">
    La scalabilit√© implique non seulement la capacit√© √† ajouter des ressources, mais aussi √† les retirer efficacement pour ma√Ætriser les co√ªts.  
    Les patterns CQRS et event sourcing sont tr√®s appr√©ci√©s dans les architectures microservices pour s√©parer les pr√©occupations.
  </aside>
</section>
<section>
  <h2>R√©silience</h2>
  <li><strong>Patterns de r√©silience</strong>
    <ul>
      <li><em>Circuit Breaker</em> : d√©tecte les appels d√©faillants, coupe la connexion pour √©viter l‚Äôengorgement.</li>
      <li><em>Retry</em> : nouvelle tentative automatique, id√©alement avec backoff exponentiel et jitter.</li>
      <li><em>Timeout</em> : arr√™t rapide des appels trop lents, √©vite de bloquer les ressources.</li>
      <li><em>Bulkhead</em> : isolation de sous-syst√®mes pour emp√™cher la propagation des pannes.</li>
    </ul>
  </li>
  <aside class="notes">
    Ces patterns permettent de g√©rer les pannes de mani√®re proactive, en √©vitant que des erreurs dans un service n'affectent l'ensemble du syst√®me.  
    Le circuit breaker est particuli√®rement utile pour √©viter les appels en cascade vers des services d√©faillants.
    Le retry est utile pour les erreurs temporaires, mais doit √™tre utilis√© avec pr√©caution pour √©viter de surcharger le service.
    Le timeout est essentiel pour √©viter que des appels bloquants n'affectent la r√©activit√© de l'application.
    Le bulkhead pattern est inspir√© des navires : compartimenter pour √©viter la propagation d'une fuite.
  </aside>
</section>
<section>
  <h2>R√©silience</h2>
  <li><strong>D√©gradation gracieuse & autogu√©rison</strong>
    <ul>
      <li><em>Graceful degradation</em> : d√©sactivation de fonctionnalit√©s non critiques (ex. retrait de certaines UI) sous forte charge.</li>
      <li><em>Self-Healing</em> : red√©marrage automatique de conteneurs d√©faillants, probes Kubernetes (liveness/readiness).</li>
      <li><em>Failover</em> : configuration active-passive ou active-active pour basculer vers un site de secours.</li>
    </ul>
  </li>
  <aside class="notes">
    La d√©gradation gracieuse permet de maintenir une partie du service op√©rationnelle m√™me en cas de surcharge.  
    L'autogu√©rison est essentielle pour r√©duire le temps d'arr√™t et garantir la disponibilit√©.
    Le failover est crucial pour les applications critiques, garantissant une continuit√© de service m√™me en cas de panne majeure.
    La configuration active-active est plus complexe mais offre une meilleure tol√©rance aux pannes.
    La configuration active-passive est plus simple mais peut entra√Æner des temps d'arr√™t lors de la bascule.
  </aside>
</section>
<section>
  <h2>R√©silience</h2>
  <li><strong>Chaos Engineering & tests de r√©silience</strong>
    <ul>
      <li>Injection de pannes (latence, coupure r√©seau) avec outils comme Chaos Monkey.</li>
      <li>Sc√©narios de basculement planifi√©s pour valider les proc√©dures de r√©cup√©ration.</li>
    </ul>
  </li>
  <aside class="notes">
    Le chaos engineering est une pratique proactive qui permet de tester la r√©silience d'un syst√®me en simulant des pannes r√©elles.  
    Cela aide √† identifier les points faibles et √† am√©liorer la robustesse de l'architecture.
    Les tests de r√©silience doivent √™tre int√©gr√©s dans le cycle de d√©veloppement pour garantir une couverture continue.
    Il est important de documenter les sc√©narios de basculement et de s'assurer que toutes les √©quipes sont form√©es aux proc√©dures de r√©cup√©ration.
    Le chaos engineering est inspir√© des pratiques militaires de simulation de crise pour pr√©parer les √©quipes √† r√©agir rapidement.
  </aside>
</section>
<section>
  <h2>R√©silience</h2>
  <li><strong>Observabilit√© pour la r√©silience</strong>
    <ul>
      <li>M√©triques (latence, taux d‚Äôerreur), logs structur√©s, traces distribu√©es (OpenTelemetry).</li>
      <li>SLO / SLA / SLIs pour d√©finir et surveiller les objectifs de fiabilit√©.</li>
      <li>Alertes intelligentes (anomalies bas√©es sur le machine learning).</li>
    </ul>
  </li>
  <aside class="notes">
    L'observabilit√© est essentielle pour comprendre le comportement du syst√®me en temps r√©el et d√©tecter les anomalies.  
    Les SLO, SLA et SLIs aident √† d√©finir des attentes claires en mati√®re de performance et de disponibilit√©.
    Les alertes intelligentes permettent de r√©duire le bruit des faux positifs et d'alerter uniquement sur des anomalies significatives.
    L'utilisation de traces distribu√©es permet de suivre le parcours d'une requ√™te √† travers plusieurs services, facilitant ainsi le diagnostic des probl√®mes.
    L'OpenTelemetry est un standard ouvert pour la collecte de m√©triques, logs et traces, facilitant l'int√©gration avec divers outils d'observabilit√©.
  </aside>
</section>
<section>
  <h2>R√©silience</h2>
  <li><strong>Conception pour l‚Äô√©chec</strong>
    <ul>
      <li>Idempotence des op√©rations pour retrys s√ªrs.</li>
      <li>D√©couplage via files d‚Äôattente (Kafka, RabbitMQ) pour absorber les pics.</li>
      <li>R√©plication multi-zones et multi-r√©gions pour tol√©rance maximale.</li>
    </ul>
  </li>
  <aside class="notes">
    Une architecture r√©siliente ne se contente pas de r√©agir aux pannes mais les anticipe et les simule.  
    L‚Äôobservabilit√© est le socle pour d√©tecter et diagnostiquer rapidement tout incident.
  </aside>
</section>

<section>
  <h2>üîó Sources</h2>
  <ul>
    <li><a href="https://en.wikipedia.org/wiki/Scalability" target="_blank">Wikipedia ‚Äì Scalability</a></li>
    <li><a href="https://martinfowler.com/articles/resilience.html" target="_blank">Martin Fowler ‚Äì Patterns for Resilience</a></li>
    <li><a href="https://microservices.io/patterns/reliability/index.html" target="_blank">Microservices.io ‚Äì Reliability Patterns</a></li>
    <li><a href="https://aws.amazon.com/autoscaling/" target="_blank">AWS ‚Äì Auto Scaling</a></li>
  </ul>
  <aside class="notes">
    Ces r√©f√©rences offrent un panorama complet des patterns et bonnes pratiques pour des architectures distribu√©es fiables et √©volutives.  
    AWS et Martin Fowler illustrent des cas concrets d‚Äôimpl√©mentation en production.
  </aside>
</section>
